batch_size: 64
num_epochs: 200
val_size: 0.15
test_size: 0.15
lr: 1e-5
num_workers: 16
label_smoothing: 0.2
tokenize_data_path: data/tokenized/train.pkl
model_name: bert-base-uncased
training_strategy:
model:
  hidden_dropout_prob: 0.6
weight_decay: 0.1
early_stopping:
  min_delta: 0.0
  patience: 10
